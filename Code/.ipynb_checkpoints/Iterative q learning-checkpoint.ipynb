{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bandit:\n",
    "    \n",
    "    def __init__(self, k):\n",
    "        \"\"\"\n",
    "        k: number of bandits \n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.mean_sd_list = [] # Storing mean and sf of each bandit\n",
    "        \n",
    "        max_mean = 0\n",
    "        self.max_i = 0\n",
    "        \n",
    "        for i in range(k):\n",
    "            mean = np.random.randint(5, 100)\n",
    "            sigma = random.uniform(0, 1)\n",
    "            self.mean_sd_list.append((mean, sigma))\n",
    "            \n",
    "            if mean > max_mean:\n",
    "                max_mean = mean\n",
    "                self.max_i = i\n",
    "        \n",
    "    def generate_reward(self, i):\n",
    "        mu, sigma = self.mean_sd_list[i]\n",
    "        return np.random.normal(mu, sigma)\n",
    "    \n",
    "    def generate_optimum_reward(self):\n",
    "        return self.generate_reward(self.max_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(object):\n",
    "    \n",
    "    def __init__(self, bandit):\n",
    "        \"\"\"\n",
    "        bandit(object of class Bandit) to solve\n",
    "        \"\"\"\n",
    "        \n",
    "        self.bandit = bandit\n",
    "        \n",
    "        self.counts = [0] * self.bandit.k\n",
    "        self.mean_estimate = [0] * self.bandit.k\n",
    "        self.actions = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.counts = [0] * self.bandit.k\n",
    "        self.mean_estimate = [0] * self.bandit.k\n",
    "        self.actions = []\n",
    "        \n",
    "        init_bandit = np.random.randint(0, self.bandit.k)\n",
    "#         print(\"Init_bandit = \", init_bandit)\n",
    "        return init_bandit\n",
    "    \n",
    "    def step(self, a):\n",
    "        '''\n",
    "        updates current state and returns next state\n",
    "        a : chosen bandit index\n",
    "        '''\n",
    "#         print(\"a = \", a)\n",
    "        reward = self.bandit.generate_reward(a)\n",
    "        self.mean_estimate[a] = (self.mean_estimate[a] * self.counts[a] + reward)/ (self.counts[a] + 1)\n",
    "        self.counts[a] += 1\n",
    "        \n",
    "        next_bandit = np.argmax(self.mean_estimate)\n",
    "        \n",
    "        return next_bandit, reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(k, bandit = None, num_episodes = 10, num_trials = 10000, compute_convergence = False, resetQ = False, debug = False, lr = 0.8, y = 0.95):\n",
    "    \"\"\"\n",
    "    k = number of bandits\n",
    "    \"\"\"\n",
    "    \n",
    "    # defining object of class Bandit \n",
    "    if not bandit:\n",
    "        bandit = Bandit(k) \n",
    "        print(\"Guassian distribution of bandits = \\n\", bandit.mean_sd_list)\n",
    "    \n",
    "    env = Environment(bandit)\n",
    "    \n",
    "    Q = np.zeros([k, k])\n",
    "    reward_list = []\n",
    "    state_transition_count = np.zeros(k)\n",
    "    convergence_episode = None\n",
    "    best_bandit = None\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        \n",
    "        if compute_convergence:\n",
    "            if resetQ:\n",
    "                Q = np.zeros([k, k])\n",
    "            state_transition_count = np.zeros(k)\n",
    "            best_bandit_so_far = None\n",
    "            best_bandit_streak = 0\n",
    "            \n",
    "        s = env.reset()\n",
    "            \n",
    "        total_reward = 0\n",
    "        j = 0\n",
    "        if debug:    print(\"--- Episode {} ---\".format(i))\n",
    "        while j < num_trials:\n",
    "            j += 1\n",
    "            \n",
    "            if np.random.random() < 0.4:\n",
    "                a = np.random.randint(0, k)\n",
    "            else:\n",
    "                a = np.argmax(Q[s, :])\n",
    "                \n",
    "            # Get new state and reward from environment\n",
    "            s1, reward = env.step(a)\n",
    "            \n",
    "#             print(\"\\ns = {}, a = {}, s1 = {}, reward = {}\".format(s, a, s1, reward))\n",
    "#             print(\"Mean estimate = \", env.mean_estimate)\n",
    "            \n",
    "            # Update Q-table with new knowledge\n",
    "            Q[s,a] = Q[s,a] + lr * (reward + y * np.max(Q[s1,:]) - Q[s,a])\n",
    "            state_transition_count[s] += 1\n",
    "            \n",
    "            total_reward += reward\n",
    "            s = s1\n",
    "             \n",
    "            '''\n",
    "            Trying different convergece conditions\n",
    "            Method 1 - \n",
    "            In each episode, if the method gives the same best bandit for {20} trials, \n",
    "            then the algorithm has converged. If the streak is broken, restart the count for the new best bandit. \n",
    "            {20} is a hyperparameter. \n",
    "            '''\n",
    "            \n",
    "#             if compute_convergence:\n",
    "#                 best_bandits = np.argmax(Q, axis = 1)\n",
    "                \n",
    "#                 single_best_bandit = np.all(best_bandits == best_bandits[0])\n",
    "#                 if single_best_bandit:\n",
    "#                     if best_bandits[0] == best_bandit_so_far:\n",
    "#                         best_bandit_streak += 1\n",
    "#                     else:\n",
    "#                         best_bandit_so_far = best_bandits[0]\n",
    "#                         best_bandit_streak = 1\n",
    "                        \n",
    "#                     if best_bandit_streak > 20:\n",
    "#                         print(\"\\n\\nQ = {}, \\nbest_bandit = {}, \\nstate_tranisition_count {}\".format(Q, best_bandits, state_transition_count))\n",
    "#                         print(\"Convergence at t = \", j)\n",
    "#                         convergence_episode = i\n",
    "#                         break\n",
    "\n",
    "        '''\n",
    "        Method 2 - \n",
    "        After each episode, if all the bandits has the hight Q value for a single bandit, \n",
    "        i.e the best decision for any bandit is to move to a single bandit then the algorithm has converged. \n",
    "        This method seems more intutive. \n",
    "        '''\n",
    "\n",
    "        if compute_convergence:\n",
    "            best_bandits = np.argmax(Q, axis = 1)\n",
    "\n",
    "            single_best_bandit = np.all(best_bandits == best_bandits[0])\n",
    "            if single_best_bandit:\n",
    "                convergence_episode = i\n",
    "                best_bandit = best_bandits[0]\n",
    "                if debug:\n",
    "                    print(\"\\n\\nQ = {}, \\nbest_bandit = {}, \\nstate_tranisition_count {}\".format(Q, best_bandits, state_transition_count))\n",
    "                break\n",
    "\n",
    "        reward_list.append(total_reward) \n",
    "\n",
    "#     print(\"\\nQ value Table : \\n\", Q)\n",
    "    if debug:\n",
    "        print(\"\\nScore over time: \", sum(reward_list)/num_episodes)\n",
    "    return convergence_episode, best_bandit\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guassian distribution of bandits = \n",
      " [(8, 0.19687189128846827), (31, 0.7999575530033356), (93, 0.2299658416800454)]\n",
      "Best bandit = 2, Converged at episode 6\n"
     ]
    }
   ],
   "source": [
    "convergence_episode, best_bandit = main(k = 3, num_episodes = 100, num_trials = 10000, compute_convergence = True, resetQ = False, debug = False)\n",
    "print(\"Best bandit = {}, Converged at episode {}\".format(best_bandit, convergence_episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Q = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guassian distribution of bandits = \n",
      " [(6, 0.047925764668003024), (93, 0.712428442313467), (96, 0.37456786473436177)]\n"
     ]
    }
   ],
   "source": [
    "# number of bandits \n",
    "k = 3\n",
    "bandit = Bandit(k) \n",
    "print(\"Guassian distribution of bandits = \\n\", bandit.mean_sd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 100\n",
    "num_trials = 1000\n",
    "resetQ = False\n",
    "avg_over_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_episodes_list = []\n",
    "for _ in range(avg_over_steps):\n",
    "    convergence_episode, best_bandit = main(k = k, bandit = bandit, num_episodes = num_episodes, num_trials = num_trials, compute_convergence = True, resetQ = resetQ)\n",
    "    convergence_episodes_list.append(convergence_episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episodes (1000 steps) for 3 bandits to converge = 27.4\n"
     ]
    }
   ],
   "source": [
    "avg_episodes = np.mean(convergence_episodes_list)\n",
    "print(\"Average episodes ({} steps) for {} bandits to converge = {}\".format(num_trials, k, avg_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset Q = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "num_trials = 30000 # setting num_trials to 27.4 * 1000 = 27400 ~ 30000\n",
    "resetQ = True\n",
    "avg_over_steps = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence_episodes_list = []\n",
    "for _ in range(avg_over_steps):\n",
    "    convergence_episode, best_bandit = main(k = k, bandit = bandit, num_episodes = num_episodes, num_trials = num_trials, compute_convergence = True, resetQ = resetQ)\n",
    "    convergence_episodes_list.append(convergence_episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average episodes (30000 steps) for 3 bandits to converge = nan\n"
     ]
    }
   ],
   "source": [
    "if None not in convergence_episodes_list: \n",
    "    avg_episodes = np.mean(convergence_episodes_list)\n",
    "else:\n",
    "    num_none = 0\n",
    "    for e in convergence_episodes_list:\n",
    "        if not e:\n",
    "            num_none += 0\n",
    "    if num_none == len(convergence_episodes_list):\n",
    "        avg_episodes = None\n",
    "    else:\n",
    "        avg_episodes = np.mean([x for x in convergence_episodes_list if x is not None])\n",
    "        \n",
    "print(\"Average episodes ({} steps) for {} bandits to converge = {}\".format(num_trials, k, avg_episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* On resetting the value of Q after each episode, the algorithm does not converge, even on increasing the number of trials and episodes. In the case of bandits, resetting Q implies that the knowledge of transition is lost when the environment is reset (mean estimate of the bandits). \n",
    "\n",
    "* One reason could be since the Q value is resetting that more number of trials might be needed in an episode to converge. Increasing num_trials from 1000 to 30000 which is the average steps needed for the algorithm to converge when Q is not resetted. Even after that, the algorithm is not converging. \n",
    "\n",
    "* It is interesting to observe that for the iterative Q learning to converge, it is necessary to reset our environment - in this case, losing the knowledge of the bandits by setting the mean estimate of the bandits back to 1. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
